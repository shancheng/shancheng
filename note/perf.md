# Common

[Latency Numbers Every Programmer Should Know](http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html)

[高并发性能调试经验分享](http://makaidong.com/chenpingzhao/356_14088.html)

# Tools

## Common tools

[iostat来对linux硬盘IO性能进行了解](http://www.php-oa.com/2009/02/03/iostat.html)

[ftrace：跟踪你的内核函数](https://zhuanlan.zhihu.com/p/33267453)

top

gstack

## oprofile

operf --pid PID

opreport -l -c

## valgrind

http://www.ibm.com/developerworks/cn/linux/l-cn-valgrind/index.html
- 使用未初始化的内存
- 内存读写越界
- 内存覆盖
- 动态内存管理错误
- 内存泄露

## gprof

http://sourceware.org/binutils/docs/gprof/

http://www.cs.utah.edu/dept/old/texinfo/as/gprof_toc.html

-pg

gprof -b <exe> gmon.out

###### [Understanding iostat](https://coderwall.com/p/utc42q/understanding-iostat)

## Visual Studio Performance Tools

## IBM Quantify

## Intel VTune


# Storage

## iostat

utilization = ( (read requests + write requests) * service time in ms / 1000 ms ) * 100%

###### [iostat -x](https://dom.as/2009/03/11/iostat/)

avgqu-sz: Very very very important value – how many requests are there in a request queue. Low = either your system is not loaded, or has serialized I/O and cannot utilize underlying storage properly. High = your software stack is scalable enough to load properly underlying I/O. Queue size equal to amount of disks means (in best case of request distribution) that all your disks are busy. Queue size higher than amount of disks means that you are already trading I/O response time for better throughput (disks can optimize order of operations if they know them beforehand, thats what NCQ – Native Command Queueing does). If one complains about I/O performance issues when avgqu-sz is lower, then it is application specific stuff, that can be resolved with more aggressive read-ahead, less fsyncs, etc. One interesting part – avqu-sz, await, svctm and %util are iterdependent ( await = avgqu-sz * svctm / (%util/100)

###### [Interpreting iostat Output](https://blog.serverfault.com/2010/07/06/777852755/)

###### [How to Read Linux Iostat’s Output and Interpret System Performance](https://www.xaprb.com/blog/2010/01/09/how-linux-iostat-computes-its-results/)

###### [Measuring & Optimizing I/O Performance](https://www.igvita.com/2009/06/23/measuring-optimizing-io-performance/)

Depending on your application you will need to focus on different metrics, but as a gentle introduction let's take a look at await, svctime and avgque:

Average queue size (avgqu-sz) is a popular metric in the DBA circles, but do be careful with it when running on a SAN or any multi-spindle device. Ideally, your queue size (avgqu-sz) for a single disk should be in single digits, which means that the underlying device is well matched to the IO load generated by the application. Conversely, if the queue size is artificially low, chances are your application code can benefit from some tuning: do less disk flushing, think about caching or buffering, or in other words, double check the assumption that IO is the bottleneck!

Average access time on our disks places some hard limits on the number of IOPs - at 5ms average, we get a very optimistic 200 req/s with no read time. Hence, if you're trying to store several hundred files a second, you might want to revisit the architecture or seriously think about switching to SSD's! Databases such as MySQL work around this constraint by minimizing the number of file handles, caching data, and using aggressive buffering techniques. Willing to potentially loose a little bit of data with InnoDB? Set flush_log_at_trx_commit to 2 to avoid flushing on every transaction in favor of a periodic one second flush.

###### [Fsync Performance on Storage Devices](https://www.percona.com/blog/2018/02/08/fsync-performance-storage-devices/)

```
#!/usr/bin/python
import os, sys, mmap
# Open a file
fd = os.open( "testfile", os.O_RDWR|os.O_CREAT|os.O_DIRECT )
m = mmap.mmap(-1, 512)
for i in range (1,1000):
   os.lseek(fd,os.SEEK_SET,0)
   m[1] = "1"
   os.write(fd, m)
   os.fsync(fd)
# Close opened file
os.close( fd )
```


# Network

[10G(82599EB) 网卡测试优化(ethtool)](http://www.tuicool.com/articles/EVRjQb)

###### Network tuning steps

Find NUMA Nodes of NICs and show results

Only bond NICs in the same NUMA Node

Choose a better bonding mode - 2 or 4 with xmit_hash_policy=layer3+4

Increase ring buffer to max

Decrease number of NIC queues to 10

Confine workload to same NUMA Node as NICs using numactl/numad

Tune NIC coalescense to increase usecs before hard interrupt

Disable C-states C1E and below in BIOS and kernel


# Web

[A curated list of Web Performance Optimization](https://github.com/davidsonfellipe/awesome-wpo#)
